CS231n 6주차 : Training NN Part 1 - Activation Function
============

**1. Today..**
-----
--------------------------

   - Activation Func
   - Data Preporcessing
   - Weight Initialization
   - Batch Normalization
   - Babysitting the Learing Process
   - Hyperparameter Opimization


**2. 강의 요약**
-----
------------------

**1)  Activation Function**
![6-2](https://user-images.githubusercontent.com/66044830/129661335-8402a212-d985-4ba9-a973-e5f2a9c86771.JPG)

    - activation function : 뉴런의 활성화 여부를 정하는 역할(다음 node에 값을 전달할지 말지)
    - 필요한 이유



* 종류
![6-1](https://user-images.githubusercontent.com/66044830/129661282-ace055c5-ea5b-4e74-95db-f3a9011e6c57.JPG)


☘ [1] Sigmoid 
      
      - 최종 출력에서만 사용(중간의 hidden layer에서는 사용x)
      - gradient vanishing 문제 : backpropagation을 하게 되면 0과 가까운 값이 계속 곱해져서 기울기가 소실됨
      - zero-centered 안됨 : zero-centered가 되지 않으면 조그만 변화에도 훅훅 바뀌기 때문에 안좋다
  
![6-3](https://user-images.githubusercontent.com/66044830/129840382-2d0936d0-51d2-4da6-aad0-6b4f301ed5bd.JPG)

      - W's gradient는 모두 양수이거나 모두 음수이게 된다. -> 매우 비효율 적(지그재그 태로 넘어가니까)

------
☘ **[2] ReLU** 

![6-4](https://user-images.githubusercontent.com/66044830/129840384-b15d26ec-991e-4710-988d-cebd4d39ba0d.JPG)      
      - Zero centered 안됨
      - 0 이하 값들을 모두 버린다는 단점이 있음

---


☘ **[3] Leaky ReLU**

![6-5](https://user-images.githubusercontent.com/66044830/129840386-ba6902a6-ee5c-4bea-b4c1-53a4dc6ef75f.JPG)
      
    - 0 이하 값들을 모두 버린다는 단점 보완


➕ P ReLU , ELU .. etc / ELU는 zero mean과 가까운 결과가 나오지만 exp 계산을 해야함

--------------------------
**2) Data Preprocessing**

![6-6](https://user-images.githubusercontent.com/66044830/129840387-0c5aca1f-0d9b-4c24-a367-c0f2aece3ce8.JPG)

보통 data preprocessing에서는 zero centered랑 normalized를 많이 쓰는데 후자는 이미지에서는 사용 x => 이미지는 0~255로 이미 범위가 정해져 있기 때문에 zero centered만 한다.

➕) 이미지에서는 잘 안쓰지만 이외에도 PCA, whitened data가 있다

-----------------
**3) Wehight Initialization**

![6-7](https://user-images.githubusercontent.com/66044830/129840389-6c925c29-e12e-47c3-b929-c0e5dbf41a87.JPG)

만약 W = 0 이면 gradient vanishing이 발생한다.

* 해결방법
        
      1. Small random numbers
          (ex) W = 0.01*np.random.randn(D,H)
          -> 하지만 네트워크가 깊어지면 문제가 됨

-----------------
**4) Batch Normalization**

![6-8](https://user-images.githubusercontent.com/66044830/129840390-646ad1fd-6190-4137-a178-64ddcaf2c900.JPG)

BN은 기본적으로 gradient vanishing이 나오지 않도록 하는 idea. 일단적으로 activation 전에 잘 분포되도록 한 다음에 activation을 진행하도록 함. 즉, 순서가 FC -> BN -> activation 이런 식으로 진행됨.

