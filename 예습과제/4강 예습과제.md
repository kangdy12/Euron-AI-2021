CS231n 4주차 : Backpropagation and NN
============

**1. Today..**
-----
--------------------------

   - Backpropagation
   - Neural Networks


**2. 강의 요약**
-----
------------------

**1) Backpropagation**
: 출력 값과 gradient를 이용하여 input에서의 gradient값을 구하는 방법. chain rule이 연쇄적으로 적용된다.

* Computational graphs
     
     : 함수의 연산과정을 그래프로 나타낸 것

![4-1](https://user-images.githubusercontent.com/66044830/129655970-8e118b6d-bf6a-47c6-a111-003e8eda8cf6.JPG)


* Example of Backpropagation
![4-2](https://user-images.githubusercontent.com/66044830/129656759-b5aa0058-b1ac-4c87-9504-33b038fffadd.JPG)

    1. 마지막 노드 f 값은 df/df이므로 1이 된다(자기자신 미분)
    2. f=qz로 정의했고, df/dz 계산
    3. z의 가중치는 q이므로 3(=5-2)이 된다
    4. 같은 방식으로 q의 가중치는 z인 -4가된다
    5. 더하기 노드인 x,y의 가중치를 구한다. 이는 df/dq*dq/dy 체인룰을 이용. df/dq = -4, dq/dy = 1이므로 결과는 -4가 된다.

**[공식]**
![4-3](https://user-images.githubusercontent.com/66044830/129657220-1ce344c6-d45a-4915-8081-5fe3ce429091.JPG)

--------------------

**2) Neural Network**

![4-4](https://user-images.githubusercontent.com/66044830/129657464-20702076-4e1b-41a3-b97a-1e017bc7333b.JPG)

> 인풋, 여러 개의 히든, 아웃풋 레이어는 전부 서로의 모든 노드에 관여하여 값을 도출한다. 이를 Fully Connected, FC라고 한다.